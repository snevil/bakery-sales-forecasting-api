{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Coffee Shop Sales Forecasting: From Data Analysis to a Deployable Model\n",
    "\n",
    "This notebook serves as a practical demonstration of an end-to-end sales forecasting pipeline, designed to help businesses like bakeries and coffee shops optimize production and minimize waste. By leveraging machine learning, this project transforms raw sales data into actionable insights and accurate future predictions.\n",
    "\n",
    "## Project Workflow\n",
    "\n",
    "The analysis follows a structured, modular approach to ensure a robust and reproducible process. The key steps covered in this notebook include:\n",
    "\n",
    "### 1. Data Ingestion and Cleaning\n",
    "We will load the raw Coffee Shop Sales dataset, clean it, and prepare it for analysis.\n",
    "\n",
    "### 2. Exploratory Data Analysis (EDA)\n",
    "We will explore the sales data to identify key trends, seasonal patterns, and other insights that will inform our feature engineering.\n",
    "\n",
    "### 3. Feature Engineering\n",
    "This is a critical step where we create new variables from the raw data. We will focus on time-based features (e.g., day of the week, month) and lagged variables to capture temporal dependencies.\n",
    "\n",
    "### 4. Model Training and Evaluation\n",
    "We will train a robust XGBoost model on the engineered dataset. We will then evaluate its performance using standard metrics to ensure its accuracy.\n",
    "\n",
    "### 5. Model Deployment Preparation\n",
    "Finally, we will save the trained model in a portable format (.pkl). This file can then be imported into a separate application (like a Streamlit dashboard) to serve real-time predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel('data/coffee_shop_sales.xlsx')\n",
    "\n",
    "# Initial data inspection\n",
    "print(\"Initial DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Analysis of Initial Dataset\n",
    "\n",
    "### Data Types\n",
    "The columns are correctly formatted, with `transaction_date` already converted to a datetime object, which is essential for time-series analysis. Other numerical fields (`transaction_id`, `transaction_qty`, etc.) and categorical fields (`product_category`, `store_location`) are also correctly identified.\n",
    "\n",
    "### Completeness\n",
    "With 149,116 entries and no null values, the dataset is complete, eliminating the need for complex missing data imputation.\n",
    "\n",
    "### Granularity\n",
    "The data is at the transaction level, providing fine-grained detail that can be aggregated to build a robust forecasting model.\n",
    "\n",
    "---\n",
    "\n",
    "Now that we've confirmed the data's quality, we can proceed with the **Exploratory Data Analysis (EDA)** in the next cell to understand the underlying trends and patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Advanced Exploratory Data Analysis (EDA)\n",
    "\n",
    "# Rename columns for easier access\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Convert date and time columns to datetime objects\n",
    "df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
    "df['transaction_time'] = pd.to_datetime(df['transaction_time'], format='%H:%M:%S').dt.time\n",
    "\n",
    "# --- Analysis 1: Daily Sales Trends ---\n",
    "# Aggregate daily sales and plot\n",
    "daily_sales = df.groupby('transaction_date')['transaction_qty'].sum().reset_index()\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_sales['transaction_date'], daily_sales['transaction_qty'], marker='o', linestyle='-', markersize=2)\n",
    "plt.title('Total Daily Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales Quantity')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Analysis 2: Sales Distribution ---\n",
    "# Plot a histogram to understand the distribution of transaction quantities\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['transaction_qty'], bins=20, kde=True)\n",
    "plt.title('Distribution of Transaction Quantities')\n",
    "plt.xlabel('Transaction Quantity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Get summary statistics\n",
    "print(\"Transaction Quantity Summary:\")\n",
    "print(df['transaction_qty'].describe())\n",
    "\n",
    "\n",
    "# --- Analysis 3: Sales by Product and Day of Week ---\n",
    "# Group by product category and day of week to see trends\n",
    "df['day_of_week'] = df['transaction_date'].dt.day_name()\n",
    "sales_by_product_day = df.groupby(['product_category', 'day_of_week'])['transaction_qty'].sum().unstack(fill_value=0)\n",
    "sales_by_product_day = sales_by_product_day.reindex(columns=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(sales_by_product_day, cmap='YlGnBu', annot=True, fmt=\".0f\", linewidths=.5)\n",
    "plt.title('Sales by Product Category and Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Product Category')\n",
    "plt.show()\n",
    "\n",
    "# --- Analysis 4: Hourly Sales ---\n",
    "# Extract hour from transaction time\n",
    "df['transaction_hour'] = pd.to_datetime(df['transaction_time'], format='%H:%M:%S').dt.hour\n",
    "hourly_sales = df.groupby('transaction_hour')['transaction_qty'].sum()\n",
    "plt.figure(figsize=(12, 6))\n",
    "hourly_sales.plot(kind='bar')\n",
    "plt.title('Total Sales by Hour of Day')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Total Sales Quantity')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Detailed EDA Findings\n",
    "\n",
    "Based on the exploratory data analysis, several key patterns and trends have been identified that will guide our forecasting model development.\n",
    "\n",
    "### Temporal Trends\n",
    "The daily sales plot reveals clear temporal trends, including an upward long-term trend and significant weekly seasonality. Sales exhibit sharp declines at the end of the month, which could suggest a specific event or a pattern related to payroll cycles.  \n",
    "The data spans a full year, but seasonal trends are not the primary drivers of large sales volumes, which seem to be more volatile.  \n",
    "This indicates the need for a model capable of capturing both **long-term trends** and **short-term volatility**.\n",
    "\n",
    "### Transaction and Sales Distribution\n",
    "The distribution of transaction quantities shows a high frequency of small purchases, with the average transaction quantity being low (**1.43**). This is consistent with a coffee shop business model.  \n",
    "The high standard deviation, however, suggests significant variability due to occasional large purchases, which represent the top 25th percentile of the data.  \n",
    "This highlights the importance of using a **robust model that is not overly sensitive to outliers**, and it could benefit from a **logarithmic transformation of the target variable**.\n",
    "\n",
    "### Category and Hourly Insights\n",
    "The sales heatmap by product category and day of the week shows that key products like **Bakery, Coffee, and Tea** maintain consistently high sales volumes throughout the week, without sharp drops.  \n",
    "Significant sales peaks are observed in the **early morning (7–10 AM)** and **late afternoon (4–6 PM)**, corresponding to the typical morning commute and afternoon break.  \n",
    "This confirms that **time-of-day features** are crucial for accurate predictions.  \n",
    "While the sales of **Packaged Chocolate** show a slight weekend peak, the overall sales volume remains static across all product categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This cell is dedicated to feature engineering, a critical step where we transform the raw data into a format that our machine learning model can learn from more effectively.\n",
    "\n",
    "Based on our EDA findings, we are creating new features to capture key patterns:\n",
    "\n",
    "### Temporal Features\n",
    "We extract calendar-related information such as **day of the week, month, and year** to account for weekly and yearly seasonality.\n",
    "\n",
    "### Lag and Rolling Features\n",
    "We create lagged features (e.g., `sales_lag_1`) and rolling averages (e.g., `rolling_mean_7_days`).  \n",
    "These features act as a \"memory\" for the model, giving it context on recent sales trends, which is crucial for time-series forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "By doing so, we are providing the model with a richer dataset, enabling it to make more accurate and informed predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Feature Engineering\n",
    "\n",
    "# Create a copy to avoid changing the original DataFrame\n",
    "df_features = df.copy()\n",
    "\n",
    "# 1. Aggregate to daily sales by product and store\n",
    "daily_product_sales = df_features.groupby(['transaction_date', 'product_id', 'store_id'])['transaction_qty'].sum().reset_index()\n",
    "\n",
    "# 2. Add Calendar Features (from EDA findings)\n",
    "daily_product_sales['day_of_week'] = daily_product_sales['transaction_date'].dt.dayofweek\n",
    "daily_product_sales['month'] = daily_product_sales['transaction_date'].dt.month\n",
    "daily_product_sales['year'] = daily_product_sales['transaction_date'].dt.year\n",
    "daily_product_sales['is_weekend'] = daily_product_sales['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# 3. Add Time-based Features (Lag and Rolling Averages)\n",
    "# Sort data for correct lag and rolling window calculations\n",
    "daily_product_sales.sort_values(by=['store_id', 'product_id', 'transaction_date'], inplace=True)\n",
    "\n",
    "# Create a 'sales_lag_1' feature (sales from the previous day)\n",
    "daily_product_sales['sales_lag_1'] = daily_product_sales.groupby(['store_id', 'product_id'])['transaction_qty'].shift(1)\n",
    "\n",
    "# Create a 'rolling_mean_7_days' feature (average sales over the last 7 days)\n",
    "daily_product_sales['rolling_mean_7_days'] = daily_product_sales.groupby(['store_id', 'product_id'])['transaction_qty'].rolling(window=7, min_periods=1).mean().reset_index(level=[0,1], drop=True)\n",
    "\n",
    "# Fill any missing values created by lags and rolling windows\n",
    "daily_product_sales.fillna(0, inplace=True)\n",
    "\n",
    "print(\"DataFrame with new features:\")\n",
    "print(daily_product_sales.head())\n",
    "print(\"\\nNew features added successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model Training and Evaluation\n",
    "\n",
    "import os\n",
    "\n",
    "# Create /model folder if it does not exist\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "features = ['day_of_week', 'month', 'year', 'is_weekend', 'sales_lag_1', 'rolling_mean_7_days']\n",
    "target = 'transaction_qty'\n",
    "\n",
    "X = daily_product_sales[features]\n",
    "y = daily_product_sales[target]\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model using Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "print(\"Model training completed.\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on validation set: {rmse:.2f}\")\n",
    "\n",
    "# Save model\n",
    "xgb_model.save_model(\"model/xgb_model_baseline.pkl\")\n",
    "print(\"XGBoost baseline model saved in /model/xgb_model_baseline.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "The baseline RMSE with validation (3.19) provides a solid starting point.  \n",
    "Considering that the average transaction quantity is **1.43 units**, an error of **3.19** is significant, yet still acceptable as an initial benchmark.  \n",
    "\n",
    "This result suggests that the model has learned the main patterns, such as **weekly seasonality** and the overall **trend**, but it is not yet sufficiently precise.  \n",
    "\n",
    "To enhance the model, we introduced **clustering as part of feature engineering**.  \n",
    "By applying **K-Means clustering** to group products with similar sales behavior, we aimed to capture additional patterns across product categories and incorporate them as new predictive features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Product Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Aggregate data by product to create clustering features\n",
    "product_features = df.groupby('product_id').agg(\n",
    "    total_sales=('transaction_qty', 'sum'),\n",
    "    average_price=('unit_price', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Handle potential outliers by scaling the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(product_features[['total_sales', 'average_price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal number of clusters using the Elbow Method\n",
    "inertia = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42, n_init='auto')\n",
    "    kmeans.fit(scaled_features)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), inertia, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with the chosen number of clusters\n",
    "optimal_clusters = 3\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init='auto')\n",
    "product_features['product_cluster'] = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    x='total_sales',\n",
    "    y='average_price',\n",
    "    hue='product_cluster',\n",
    "    data=product_features,\n",
    "    palette='viridis',\n",
    "    s=100\n",
    ")\n",
    "plt.title(f'Product Clusters (K={optimal_clusters})')\n",
    "plt.xlabel('Total Sales (Scaled)')\n",
    "plt.ylabel('Average Price (Scaled)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the cluster labels back into the original dataframe\n",
    "df = df.merge(product_features[['product_id', 'product_cluster']], on='product_id', how='left')\n",
    "print(\"DataFrame with new 'product_cluster' feature:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of products in each cluster\n",
    "print(\"\\nProduct Cluster Distribution:\")\n",
    "cluster_summary = df.groupby('product_cluster')['product_detail'].unique()\n",
    "for cluster_id, products in cluster_summary.items():\n",
    "    print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "    print(f\"Number of unique products: {len(products)}\")\n",
    "    print(f\"Sample products: {products[:5].tolist()}\") # Show only the first 5 products for brevity\n",
    "# Save the DataFrame with product clusters to a CSV file\n",
    "product_features.to_csv('product_clusters.csv', index=False)\n",
    "\n",
    "print(\"I dati dei cluster sono stati salvati nel file 'product_clusters.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Analysis of Product Clusters\n",
    "\n",
    "The product clustering analysis successfully divided the products into three distinct groups based on their sales behavior, providing a deeper understanding of our product portfolio. This is a crucial step in advanced feature engineering for our forecasting model.\n",
    "\n",
    "### Cluster 0 – High Volume, Low Price\n",
    "- **56 unique products**\n",
    "- Includes high-demand items like *Our Old Time Diner Blend Sm* and *Oatmeal Scone*  \n",
    "- Represents the **core business cluster**, driving the majority of sales volume  \n",
    "- Sales are likely influenced by **daily and weekly routines**  \n",
    "- Forecasting this group should be the model’s **primary focus**\n",
    "\n",
    "### Cluster 1 – Low Volume, Medium-High Price\n",
    "- **23 products**\n",
    "- Includes items like *Ethiopia* and *Peppermint*  \n",
    "- Represents **premium or niche products**, sold in smaller quantities at higher prices  \n",
    "- A separate forecasting strategy or specialized model may be needed to avoid diluting the accuracy of high-volume products\n",
    "\n",
    "### Cluster 2 – Outlier\n",
    "- **1 product: Civet Cat**  \n",
    "- Distinguished by **extremely high price** and **very low sales volume**  \n",
    "- Needs to be treated separately to prevent skewing overall model performance\n",
    "\n",
    "---\n",
    "\n",
    "By incorporating this clustering analysis, we introduced a new categorical feature (`product_cluster`) into the XGBoost model.  \n",
    "This allows the model to recognize that products within the same cluster share similar sales dynamics, enabling it to generalize better and potentially improve predictive accuracy, especially for less common items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training pipeline with product clusters\n",
    "\n",
    "import os\n",
    "\n",
    "# Create /model folder if it does not exist\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# Aggregate daily sales per product and cluster\n",
    "daily_product_sales = (\n",
    "    df.groupby(['transaction_date', 'product_id', 'store_id', 'product_cluster'])['transaction_qty']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# One-hot encoding for product clusters\n",
    "daily_product_sales = pd.get_dummies(daily_product_sales, columns=['product_cluster'], prefix='cluster')\n",
    "\n",
    "# Temporal features\n",
    "daily_product_sales['transaction_date'] = pd.to_datetime(daily_product_sales['transaction_date'])\n",
    "daily_product_sales['year'] = daily_product_sales['transaction_date'].dt.year\n",
    "daily_product_sales['month'] = daily_product_sales['transaction_date'].dt.month\n",
    "daily_product_sales['day_of_week'] = daily_product_sales['transaction_date'].dt.dayofweek\n",
    "daily_product_sales['is_weekend'] = (daily_product_sales['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Lag and rolling mean features\n",
    "daily_product_sales = daily_product_sales.sort_values('transaction_date')\n",
    "daily_product_sales['sales_lag_1'] = daily_product_sales.groupby('product_id')['transaction_qty'].shift(1)\n",
    "daily_product_sales['rolling_mean_7_days'] = (\n",
    "    daily_product_sales.groupby('product_id')['transaction_qty']\n",
    "    .rolling(7, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# Drop rows with NaN from lag/rolling\n",
    "daily_product_sales = daily_product_sales.dropna(subset=['sales_lag_1', 'rolling_mean_7_days'])\n",
    "\n",
    "# Features and target\n",
    "cluster_cols = [c for c in daily_product_sales.columns if c.startswith('cluster_')]\n",
    "features = ['day_of_week', 'month', 'year', 'is_weekend', 'sales_lag_1', 'rolling_mean_7_days'] + cluster_cols\n",
    "target = 'transaction_qty'\n",
    "\n",
    "X = daily_product_sales[features]\n",
    "y = daily_product_sales[target]\n",
    "\n",
    "# Train/validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost model with clusters\n",
    "import xgboost as xgb\n",
    "xgb_model_v2 = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model_v2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_v2 = xgb_model_v2.predict(X_val)\n",
    "rmse_v2 = np.sqrt(mean_squared_error(y_val, y_pred_v2))\n",
    "\n",
    "print(f\"RMSE without clusters: 3.19\")\n",
    "print(f\"RMSE with clusters: {rmse_v2:.3f}\")\n",
    "\n",
    "# Save model\n",
    "xgb_model_v2.save_model(\"model/xgb_model_clusters.pkl\")\n",
    "print(\"XGBoost model saved in /model/xgb_model_clusters.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Interpretation of Results: Impact of Product Clusters\n",
    "\n",
    "The RMSE without clusters was **3.19**, while with clusters it only decreased slightly to **~3.1x**.  \n",
    "This small reduction indicates that **product clusters did not add significant predictive power** to the model.\n",
    "\n",
    "### Why is the impact so small?\n",
    "1. **Clusters add little new information**  \n",
    "   The sales history itself (lags and rolling averages) already captures most of the product dynamics. Static groupings like clusters cannot explain daily fluctuations.\n",
    "\n",
    "2. **High noise at daily level**  \n",
    "   Daily sales often show strong variability due to promotions, local events, weather, or stock availability. Such external shocks cannot be explained by a static \"cluster\" variable.\n",
    "\n",
    "3. **Temporal features dominate**  \n",
    "   Lag and rolling statistics of past sales are much stronger predictors than cluster membership. The model essentially \"learns\" product similarity from sales history alone.\n",
    "\n",
    "### What does this mean?\n",
    "- Clustering may still make sense for **business analysis** (e.g., product segmentation, assortment planning), but for **short-term forecasting** it has little marginal benefit.  \n",
    "- The model's predictive power is mainly driven by **temporal dynamics** (lags, rolling means, seasonality).  \n",
    "\n",
    "### What could improve the model?\n",
    "- **Richer temporal features**:  \n",
    "  Add more calendar-based signals (holiday flags, month-end effects, seasonal indicators).  \n",
    "- **External data**:  \n",
    "  Weather, promotions, marketing campaigns, or local events would likely explain part of the unexplained variance.  \n",
    "- **Product-level attributes**:  \n",
    "  If available, continuous features like price, margin, or product lifecycle stage may be more informative than categorical clusters.\n",
    "\n",
    "---\n",
    "\n",
    " **Conclusion:**  \n",
    "The minimal RMSE reduction shows that **clustering products does not materially improve short-term forecasts** in this dataset. Forecast accuracy will benefit more from **additional temporal and external features** rather than static product segmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Next Steps: Enriching Features\n",
    "\n",
    "The time series plot of total daily sales shows a **clear upward trend** and **high daily variability**.  \n",
    "This explains why product clusters had minimal impact: most of the predictive power comes from **temporal patterns** and external factors.\n",
    "\n",
    "### Additional features to improve forecasting:\n",
    "1. **Calendar features**\n",
    "   - Day of month, quarter, start/end of month.\n",
    "   - Holiday flags (public holidays, weekends).\n",
    "   - Promotional day flags if available.\n",
    "\n",
    "2. **Extended lag and rolling windows**\n",
    "   - Lags of 7 and 30 days.\n",
    "   - Rolling averages over 14 or 30 days.\n",
    "   - Rolling volatility (standard deviation) to capture demand uncertainty.\n",
    "\n",
    "3. **Interaction features**\n",
    "   - Cross-effects between lags and day of week.\n",
    "   - Product cluster × seasonality (e.g. summer products in summer).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    " **Conclusion:**  \n",
    "To significantly reduce RMSE, the model should capture **seasonality, calendar effects, and external drivers** of demand. Product clusters alone are not sufficient for short-term forecasting accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training pipeline with clusters + extended lag features\n",
    "\n",
    "import os\n",
    "\n",
    "# Create /model folder if it does not exist\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# Aggregate daily sales per product and cluster\n",
    "daily_product_sales = (\n",
    "    df.groupby(['transaction_date', 'product_id', 'store_id', 'product_cluster'])['transaction_qty']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# One-hot encoding for clusters\n",
    "daily_product_sales = pd.get_dummies(daily_product_sales, columns=['product_cluster'], prefix='cluster')\n",
    "\n",
    "# Temporal features\n",
    "daily_product_sales['transaction_date'] = pd.to_datetime(daily_product_sales['transaction_date'])\n",
    "daily_product_sales['year'] = daily_product_sales['transaction_date'].dt.year\n",
    "daily_product_sales['month'] = daily_product_sales['transaction_date'].dt.month\n",
    "daily_product_sales['day_of_week'] = daily_product_sales['transaction_date'].dt.dayofweek\n",
    "daily_product_sales['is_weekend'] = (daily_product_sales['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Sort by date\n",
    "daily_product_sales = daily_product_sales.sort_values('transaction_date')\n",
    "\n",
    "# Lag features\n",
    "daily_product_sales['sales_lag_1']  = daily_product_sales.groupby('product_id')['transaction_qty'].shift(1)\n",
    "daily_product_sales['sales_lag_7']  = daily_product_sales.groupby('product_id')['transaction_qty'].shift(7)\n",
    "daily_product_sales['sales_lag_30'] = daily_product_sales.groupby('product_id')['transaction_qty'].shift(30)\n",
    "\n",
    "# Rolling mean features\n",
    "daily_product_sales['rolling_mean_7']  = (\n",
    "    daily_product_sales.groupby('product_id')['transaction_qty']\n",
    "    .rolling(7, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    ")\n",
    "daily_product_sales['rolling_mean_14'] = (\n",
    "    daily_product_sales.groupby('product_id')['transaction_qty']\n",
    "    .rolling(14, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    ")\n",
    "daily_product_sales['rolling_mean_30'] = (\n",
    "    daily_product_sales.groupby('product_id')['transaction_qty']\n",
    "    .rolling(30, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# Drop rows with NaN introduced by lags\n",
    "daily_product_sales = daily_product_sales.dropna(subset=['sales_lag_1','sales_lag_7','sales_lag_30'])\n",
    "\n",
    "# Features and target\n",
    "cluster_cols = [c for c in daily_product_sales.columns if c.startswith('cluster_')]\n",
    "features = [\n",
    "    'day_of_week','month','year','is_weekend',\n",
    "    'sales_lag_1','sales_lag_7','sales_lag_30',\n",
    "    'rolling_mean_7','rolling_mean_14','rolling_mean_30'\n",
    "] + cluster_cols\n",
    "target = 'transaction_qty'\n",
    "\n",
    "X = daily_product_sales[features]\n",
    "y = daily_product_sales[target]\n",
    "\n",
    "# Train/val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost model\n",
    "import xgboost as xgb\n",
    "xgb_model_v3 = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model_v3.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_v3 = xgb_model_v3.predict(X_val)\n",
    "rmse_v3 = np.sqrt(mean_squared_error(y_val, y_pred_v3))\n",
    "\n",
    "print(f\"RMSE without clusters: 3.19\")\n",
    "print(f\"RMSE with clusters + extended lags: {rmse_v3:.3f}\")\n",
    "\n",
    "# Save model\n",
    "xgb_model_v3.save_model(\"model/xgb_model_clusters_lags.pkl\")\n",
    "print(\"XGBoost model saved in /model/xgb_model_clusters_lags.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Interpretation of Extended Lags + Clusters\n",
    "\n",
    "- **Baseline RMSE (no clusters):** 3.19  \n",
    "- **With clusters + extended lags:** 3.278  \n",
    "\n",
    "### Key Insights\n",
    "1. **No improvement** – Adding clusters and long lags actually increased RMSE.  \n",
    "2. **Reason:** The dataset is relatively short (6 months), and long lags (7, 30 days) introduce noise rather than signal.  \n",
    "3. **Clusters remain uninformative** – Sales history already captures product similarity, so static cluster features do not help.  \n",
    "4. **Trend dominates** – The strong upward trend in sales is not explained by clusters or simple lags.  \n",
    "\n",
    "### Next Steps\n",
    "- Use **time-based splits** instead of random `train_test_split` to avoid leakage.  \n",
    "- Keep only the most stable features (`lag_1`, `rolling_mean_7`).  \n",
    "- Add a **trend variable** (`days_since_start`) to explicitly model growth.  \n",
    "- Explore **time series models** (Prophet, SARIMAX) that are designed to capture seasonality and trend.  \n",
    "\n",
    " **Conclusion:** More features ≠ better performance. In this dataset, simple short-term lags outperform clusters and long lags. The next gain will come from **trend/seasonality modeling** and **external signals**, not from extra lag features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Exploring Alternative Models\n",
    "\n",
    "Since XGBoost with clusters and lags did not improve performance, it is worth exploring **time-series specific models**:\n",
    "\n",
    "1. **Prophet (Meta)**\n",
    "   - Automatically models trend and seasonality.\n",
    "   - Easy to include holidays and external regressors.\n",
    "   - Well-suited for short daily series.\n",
    "\n",
    "2. **SARIMAX**\n",
    "   - Traditional statistical approach for autoregressive patterns.\n",
    "   - Captures short lags and weekly seasonality explicitly.\n",
    "   - Provides interpretable coefficients.\n",
    "\n",
    "3. **LSTM / RNN**\n",
    "   - Useful if multiple product-level series are available.\n",
    "   - Learns complex sequential patterns.\n",
    "   - Requires larger datasets to avoid overfitting.\n",
    "\n",
    " **Conclusion:**  \n",
    "For this dataset (6 months of daily sales), Prophet or SARIMAX are more promising than XGBoost.  \n",
    "Next step: benchmark Prophet vs SARIMAX and compare RMSE against the current baseline (3.19).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Create /model folder if it does not exist\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# Prepare aggregated daily sales (all products combined)\n",
    "agg_sales = (\n",
    "    df.groupby('transaction_date')['transaction_qty']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "agg_sales.rename(columns={'transaction_date': 'ds', 'transaction_qty': 'y'}, inplace=True)\n",
    "\n",
    "# Train-validation split (last 20% as validation)\n",
    "split_index = int(len(agg_sales) * 0.8)\n",
    "train_df = agg_sales.iloc[:split_index]\n",
    "val_df   = agg_sales.iloc[split_index:]\n",
    "\n",
    "# Fit Prophet on aggregated sales\n",
    "model = Prophet(yearly_seasonality=False, weekly_seasonality=True, daily_seasonality=False)\n",
    "model.fit(train_df)\n",
    "\n",
    "# Forecast including validation period\n",
    "future = model.make_future_dataframe(periods=len(val_df))\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Extract predicted values for validation set\n",
    "y_true = val_df['y'].values\n",
    "y_pred = forecast.iloc[-len(val_df):]['yhat'].values\n",
    "\n",
    "# Compute RMSE for aggregated model\n",
    "rmse_prophet_agg = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f\"Prophet RMSE (aggregated model): {rmse_prophet_agg:.3f}\")\n",
    "\n",
    "# Save global aggregated Prophet model\n",
    "with open(\"model/prophet_aggregated.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Prophet aggregated model saved in /model/prophet_aggregated.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Prophet vs XGBoost – Product-level Comparison\n",
    "\n",
    "- **XGBoost RMSE (baseline):** 3.19  \n",
    "- **Prophet average RMSE (per product):** 6.26  \n",
    "\n",
    "### Interpretation\n",
    "- XGBoost clearly outperforms Prophet for product-level forecasting.  \n",
    "- Reason: XGBoost leverages **short-term lags and rolling averages**, which capture the local dynamics of sales better than Prophet.  \n",
    "- Prophet struggles with short and noisy product-level time series.  \n",
    "- However, Prophet remains valuable for **macro forecasting** (aggregate demand, long-term trend, seasonality) thanks to its interpretable components and uncertainty intervals.  \n",
    "\n",
    " **Conclusion:**  \n",
    "For **product-level daily forecasting**, tree-based models (XGBoost, LightGBM, CatBoost) are more accurate.  \n",
    "For **aggregate or business-level forecasting**, Prophet can provide useful insights into trend and seasonality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create /model directory if it doesn't exist\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "\n",
    "# Aggregate daily sales\n",
    "ts = df.groupby('transaction_date')['transaction_qty'].sum()\n",
    "\n",
    "# Train-validation split (last 20% = validation)\n",
    "split_index = int(len(ts) * 0.8)\n",
    "train_ts = ts.iloc[:split_index]\n",
    "val_ts   = ts.iloc[split_index:]\n",
    "\n",
    "# Fit SARIMAX on training data\n",
    "model = SARIMAX(train_ts, order=(2,1,2), seasonal_order=(1,1,1,7))\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "# Forecast horizon = length of validation\n",
    "forecast = results.get_forecast(steps=len(val_ts))\n",
    "y_pred = forecast.predicted_mean\n",
    "y_true = val_ts.values\n",
    "\n",
    "# RMSE\n",
    "rmse_sarimax = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f\"SARIMAX RMSE: {rmse_sarimax:.3f}\")\n",
    "\n",
    "# Confidence intervals\n",
    "conf_int = forecast.conf_int()\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train_ts.index, train_ts, label='Train')\n",
    "plt.plot(val_ts.index, val_ts, label='Validation', color='orange')\n",
    "plt.plot(val_ts.index, y_pred, label='SARIMAX Forecast', color='green')\n",
    "plt.fill_between(val_ts.index, conf_int.iloc[:,0], conf_int.iloc[:,1], \n",
    "                 color='green', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.title(\"SARIMAX Forecast with Validation\")\n",
    "plt.show()\n",
    "\n",
    "# Save the in /model directory\n",
    "results.save('model/sarimax_model.pkl')\n",
    "print(\"SARIMAX model saved to 'model/sarimax_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "##  Model Comparison: XGBoost, Prophet, SARIMAX\n",
    "\n",
    "### RMSE Results\n",
    "| Model                                | Level              | RMSE    |\n",
    "|--------------------------------------|--------------------|---------|\n",
    "| **XGBoost (baseline)**               | Product-level      | 3.19    |\n",
    "| **XGBoost (with clusters)**          | Product-level      | 3.15    |\n",
    "| **Prophet (average across products)**| Product-level      | 6.26    |\n",
    "| **Prophet (aggregate)**              | Total daily sales  | 236.8   |\n",
    "| **SARIMAX (aggregate)**              | Total daily sales  | 358.4   |\n",
    "\n",
    "---\n",
    "\n",
    "###  Interpretation\n",
    "\n",
    "- **XGBoost (baseline vs clusters)**  \n",
    "  - Baseline RMSE: **3.19**  \n",
    "  - With clusters: **3.15**  \n",
    "  - The inclusion of clusters produced only a **marginal improvement**, confirming that short-term lags and rolling features dominate predictive power.  \n",
    "\n",
    "- **Prophet**  \n",
    "  - Performs poorly at product-level (RMSE ~6.26), due to short and noisy individual series.  \n",
    "  - Performs better at aggregate level (RMSE ~236.8), effectively capturing **trend and weekly seasonality**.  \n",
    "  - Useful for interpretable long-term, macro-level forecasting.  \n",
    "\n",
    "- **SARIMAX**  \n",
    "  - RMSE ~358.4 at aggregate level, worse than Prophet.  \n",
    "  - Captures some trend and seasonality, but less accurate and less flexible than Prophet on this dataset.  \n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusions\n",
    "\n",
    "- For **product-level forecasting (micro)**:  \n",
    "  ➝ **XGBoost** is the most accurate model. Clusters add negligible value, while **lag/rolling features are the key drivers** of performance.  \n",
    "\n",
    "- For **aggregate forecasting (macro)**:  \n",
    "  ➝ **Prophet** is preferable to SARIMAX, offering clearer trend/seasonality decomposition and better accuracy.  \n",
    "\n",
    "- **SARIMAX** serves as a useful statistical benchmark but underperforms compared to Prophet in this case.\n",
    "\n",
    " **Overall:**  \n",
    "- Use **XGBoost** when precision at the product level is required.  \n",
    "- Use **Prophet** when analyzing global sales trends and seasonality at the business level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
